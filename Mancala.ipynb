{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Game Engine\n",
    "\n",
    "class Mancala:\n",
    "    import random\n",
    "    def __init__(self, disp = False):\n",
    "        # Initialize Mancala game board\n",
    "        self.board = np.ones((2,6), dtype=int) * 4\n",
    "        \n",
    "        # Initialize Scores to 0\n",
    "        self.score = [0, 0]\n",
    "        \n",
    "        # Choose which player goes first (probably change to random initialization)\n",
    "        self.turn = random.randint(0, 1)    # 0 for TOP player\n",
    "                                            # 1 for BOTTOM player\n",
    "            \n",
    "        # Game End indicator\n",
    "        self.game_end = False\n",
    "        \n",
    "        # Should prints be displayed?\n",
    "        self.disp = disp\n",
    "        \n",
    "    \n",
    "    ## def output\n",
    "    def __str__(self):\n",
    "        return f'Score TOP Player: {self.score[0]}    Score BOTTOM Player:  {self.score[1]} \\n{self.board} \\nNext turn: Player {\"BOTTOM\" if self.turn else \"TOP\"}'\n",
    "        \n",
    "    \n",
    "    def seed(self, seed):\n",
    "        random.seed(seed)\n",
    "    \n",
    "    ## act on the game\n",
    "    def action(self, action):\n",
    "        # Action meanings:\n",
    "        # 0-5:  distribute beans from bowl 0-5 WITHOUT placing one in the Score\n",
    "        # 6-11: distribute beans from bowl 0-5 AND place one in the Score\n",
    "        \n",
    "        # interpret input\n",
    "        bowl = action % 6      # take beans from bowl\n",
    "        toScore = action // 6  # 0 -> skip Score, 1 -> do not skip Score\n",
    "        \n",
    "        # position\n",
    "        x = abs(5*(1 - self.turn) - bowl)\n",
    "        y = self.turn\n",
    "        \n",
    "        # Init reward\n",
    "        reward = 0\n",
    "        \n",
    "        # How many beans are in chosen bowl\n",
    "        beans = self.board[y, x]\n",
    "        \n",
    "        # Who will be next?\n",
    "        next_turn = 1 - self.turn\n",
    "        \n",
    "        # If chosen bowl is empty\n",
    "        if beans == 0:\n",
    "            if self.disp: print(\"Error! You chose an empty bowl! -2 Points Penalty\")\n",
    "            reward -= 2\n",
    "            next_turn = 1 - next_turn\n",
    "            \n",
    "        # Take beans out of bowl\n",
    "        self.board[y, x] = 0\n",
    "        \n",
    "        # Beans are distributed counterclock-wise\n",
    "        direction = 2*self.turn - 1   # +1 -> right, -1 -> left\n",
    "        \n",
    "        # While there are beans left, distribute them counterclock-wise\n",
    "        while beans != 0:\n",
    "            \n",
    "            # If beans < 0 then there has been some error\n",
    "            if beans < 0:\n",
    "                if self.disp: print(\"Error nr of beans negative!\")\n",
    "                \n",
    "            # move to next bowl\n",
    "            x += direction  \n",
    "            \n",
    "            # if end is reached check if toScore is true and continue on other side\n",
    "            if x < 0 or x > 5:\n",
    "                # if toScore is true, put one bean in score (only if it is the correct Score)\n",
    "                if toScore and y == self.turn:\n",
    "                    reward += 1\n",
    "                    self.score[self.turn] += 1\n",
    "                    beans -= 1\n",
    "                    \n",
    "                    # if this was the last bean the same player will play again\n",
    "                    if beans == 0:\n",
    "                        next_turn = self.turn\n",
    "                        break\n",
    "                # change row and direction\n",
    "                y = 1 - y\n",
    "                direction = -direction\n",
    "                x += direction\n",
    "            \n",
    "            # if this was the last bean, check opposite bowl\n",
    "            if beans == 1 and self.board[y, x] == 0 and y == self.turn:\n",
    "                reward += 1 + self.board[1 - y, x]\n",
    "                self.score[self.turn] += 1 + self.board[1 - y, x]\n",
    "                self.board[1 - y, x] = 0\n",
    "            else:\n",
    "                self.board[y, x] += 1\n",
    "            beans -= 1\n",
    "            \n",
    "        # add reward to Score of current Player\n",
    "        #self.score[self.turn] += reward\n",
    "        \n",
    "        # end_check\n",
    "        reward += self.end_check()\n",
    "        \n",
    "        # set next player\n",
    "        self.turn = next_turn\n",
    "        \n",
    "        return reward\n",
    "       \n",
    "    def end_check(self):\n",
    "        if all(self.board[self.turn] == 0):\n",
    "            if self.disp: print(f'No more moves for {\"BOTTOM\" if self.turn else \"TOP\"} Player! \\nGame End\\n ==============================')\n",
    "            self.score[1 - self.turn] += sum(self.board[1 - self.turn])\n",
    "            loose_penalty = -sum(self.board[1 - self.turn])\n",
    "            if self.disp: print(f'Score TOP Player: {self.score[0]}    Score BOTTOM Player:  {self.score[1]}')\n",
    "            if self.score[1] == self.score[0]:\n",
    "                if self.disp: print(\"This game is a draw!\")\n",
    "            else:\n",
    "                if self.disp: print(f'{\"BOTTOM\" if self.score[1] > self.score[0] else \"TOP\"} Player wins!')\n",
    "            self.game_end = True\n",
    "            return loose_penalty\n",
    "        else:\n",
    "            self.game_end = False\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    def human_play(self):\n",
    "        while not self.game_end:\n",
    "            print(self)\n",
    "            move = int(input(f'Whats {\"BOTTOM\" if self.turn else \"TOP\"} Players next move? (input 0-11)'))\n",
    "            if self.disp: print(f'You chose {move}\\n')\n",
    "            if move not in range(12):\n",
    "                if self.disp: print(f'{move} is an invalid input. Game Ends')\n",
    "                self.game_end = True\n",
    "                if self.disp: print(self)\n",
    "                if self.disp: print(f'Score TOP Player: {self.score[0]}    Score BOTTOM Player:  {self.score[1]}')\n",
    "                if self.score[1] == 24:\n",
    "                    if self.disp: print(\"This game is a draw!\")\n",
    "                else:\n",
    "                    if self.disp: print(f'{\"BOTTOM\" if self.score[1] > self.score[0] else \"TOP\"} Player wins!')\n",
    "                continue\n",
    "            self.action(move)\n",
    "            \n",
    "    def copy(self):\n",
    "        cop = Mancala()\n",
    "        cop.board = np.array(self.board)\n",
    "        cop.score = list(self.score)\n",
    "        cop.game_end = bool(self.game_end)\n",
    "        cop.turn = int(self.turn)\n",
    "        return cop\n",
    "        \n",
    "    def reset(self, disp = False):\n",
    "        # Initialize Mancala game board\n",
    "        self.board = np.ones((2,6), dtype=int) * 4\n",
    "        \n",
    "        # Initialize Scores to 0\n",
    "        self.score = [0, 0]\n",
    "        \n",
    "        # Choose which player goes first (probably change to random initialization)\n",
    "        self.turn = random.randint(0, 1)    # 0 for TOP player\n",
    "                                         # 1 for BOTTOM player\n",
    "            \n",
    "        # Game End indicator\n",
    "        self.game_end = False\n",
    "        \n",
    "        # Should prints be displayed?\n",
    "        self.disp = disp\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score TOP Player: 0    Score BOTTOM Player:  0 \n",
      "[[4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4]] \n",
      "Next turn: Player BOTTOM\n"
     ]
    }
   ],
   "source": [
    "game = Mancala()\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.human_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Deep-Q-Learning we need a replay memory\n",
    "class Memory:\n",
    "    def __init__(self, maxlen = 1e5):\n",
    "        self.size = [0, 0]\n",
    "        self.memory = [[],[]]\n",
    "        print(\"Memory Initialized\")\n",
    "        self.maxlen = maxlen\n",
    "        self.current_pos = [0, 0]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, turn, idx):\n",
    "        return self.memory[turn][idx]\n",
    "    \n",
    "    def draw(self, turn):\n",
    "        if self.size[turn]: # self.size should not be 0\n",
    "            return self.__getitem__(turn, random.randint(0, self.size[turn] - 1))\n",
    "        else:\n",
    "            print(f'Memory {turn} is not yet filled')\n",
    "            return [[],[]]\n",
    "        \n",
    "    def draw_batch(self, turn, batch_size):\n",
    "        batch_size = min(batch_size, self.size[turn])\n",
    "        return [self.draw(turn) for i in range(batch_size)]\n",
    "    \n",
    "    def add(self, turn, game0, action, reward, game1):\n",
    "        if self.size[turn] < self.maxlen:\n",
    "            # relevant quantities: state0, action, reward, state1, game_end\n",
    "            self.memory[turn].append([game0.board.reshape(1,12), action, reward, game1.board.reshape(1,12), game1.game_end])\n",
    "            self.size[turn] += 1\n",
    "            self.current_pos[turn] += 1\n",
    "        else:\n",
    "            self.current_pos[turn] = self.current_pos[turn] % self.maxlen\n",
    "            self.memory[turn][self.current_pos[turn]] = [game0.board.reshape(1,12), action, reward, game1.board.reshape(1,12), game1.game_end]\n",
    "            self.current_pos[turn] += 1\n",
    "        \n",
    "        \n",
    "    def add_random_game(self):\n",
    "        # Initialize game\n",
    "        game = Mancala(False)  \n",
    "        \n",
    "        # Start game Loop\n",
    "        while not game.game_end:\n",
    "            \n",
    "            # state t\n",
    "            game0 = game.copy()\n",
    "            \n",
    "            # random action\n",
    "            ac = random.randint(0, 11)\n",
    "            \n",
    "            reward = game.action(ac)\n",
    "            \n",
    "            # save state(t), action, state(t+1)\n",
    "            self.add(game0.turn, game0, ac, reward, game.copy())  \n",
    "            \n",
    "    def __str__(self):\n",
    "        return f'Size: {self.size} \\nFirst Entry Player 0:\\n {self.memory[0][0]} \\n... \\nLast Entry Player 1:\\n {self.memory[1][self.size[1] - 1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Initialized\n",
      "Size: [5, 5] \n",
      "First Entry Player 0:\n",
      " [array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]), 5, -2, array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]), False] \n",
      "... \n",
      "Last Entry Player 1:\n",
      " [array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]), 10, -2, array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]), False]\n"
     ]
    }
   ],
   "source": [
    "game = Mancala()\n",
    "mem = Memory(5)\n",
    "mem.add_random_game()\n",
    "mem.add_random_game()\n",
    "print(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]),\n",
       "   5,\n",
       "   -2,\n",
       "   array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]),\n",
       "   6,\n",
       "   -2,\n",
       "   array([[0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0]]),\n",
       "   True],\n",
       "  [array([[0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   3,\n",
       "   -2,\n",
       "   array([[0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   11,\n",
       "   -2,\n",
       "   array([[0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   10,\n",
       "   1,\n",
       "   array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False]],\n",
       " [[array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   3,\n",
       "   -2,\n",
       "   array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   5,\n",
       "   -2,\n",
       "   array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   7,\n",
       "   3,\n",
       "   array([[0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[1, 1, 0, 2, 0, 1, 1, 1, 2, 0, 0, 0]]),\n",
       "   0,\n",
       "   0,\n",
       "   array([[1, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False],\n",
       "  [array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   10,\n",
       "   -2,\n",
       "   array([[0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0]]),\n",
       "   False]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### \n",
    "### Neural Networks inspired from nndl_2020_lab_07_deep_reinforcement_learning_with_solutions.ipynb\n",
    "### From University of Padova NN&DL course winter semester 2020-21\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The policy network takes a state as input, and provides the Q-value for each of the possible actions.\n",
    "\n",
    "#Let's define a simple generic fully-connected feed forward network with state_space_dim inputs and action_space_dim outputs (e.g. 2 hidden layers with 64 neurons each).\n",
    "\n",
    "#Be sure to keep a linear output activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "                nn.Linear(state_space_dim, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, action_space_dim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example network\n",
    "net = DQN(state_space_dim=12, action_space_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epsilon Greedy Strategy. With prob. epsilon choose a random action!\n",
    "## Otherwise choose action with maximal Q value according to the DQN\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_del = 1e-4\n",
    "epsilon_min = 0.05\n",
    "\n",
    "## An alternative Strategy would be a softmax strategy. \n",
    "## Calculate the softmax of the DQN ouputs for some softmax temperature T\n",
    "## Draw an action according to the probabilities given by the softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### PARAMETERS\n",
    "gamma = 0.97   # gamma parameter for the long term reward\n",
    "replay_memory_capacity = 20000   # Replay memory capacity\n",
    "lr = 1e-2   # Optimizer learning rate\n",
    "target_net_update_steps = 1000   # Number of episodes to wait before updating the target network\n",
    "batch_size = 128   # Number of samples to take from the replay memory for each update\n",
    "bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "min_samples_for_training = 5000   # Minimum samples in the replay memory to enable the training\n",
    "num_iterations = 10000 # Number of Iterations\n",
    "\n",
    "### DQN In&Output Dimension\n",
    "state_space_dim = 12\n",
    "action_space_dim = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Initialized\n"
     ]
    }
   ],
   "source": [
    "### Initialize the replay memory\n",
    "replay_mem = Memory(replay_memory_capacity)    \n",
    "\n",
    "### Initialize the policy network\n",
    "policy_net_Bottom = DQN(state_space_dim, action_space_dim)\n",
    "policy_net_Top = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "### Initialize the target network with the same weights of the policy network\n",
    "target_net_Bottom = DQN(state_space_dim, action_space_dim)\n",
    "target_net_Bottom.load_state_dict(policy_net_Bottom.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "target_net_Top = DQN(state_space_dim, action_space_dim)\n",
    "target_net_Top.load_state_dict(policy_net_Top.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "### Initialize the optimizer\n",
    "optimizer_Bottom = torch.optim.SGD(policy_net_Bottom.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "optimizer_Top = torch.optim.SGD(policy_net_Top.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "### Initialize the loss function (Huber loss)\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    replay_mem.add_random_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4261, 4365]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_mem.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(turn, policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "    # Sample the data from the replay memory\n",
    "    batch = replay_mem.draw_batch(turn, batch_size)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Create tensors for each element of the batch\n",
    "    states      = torch.tensor([s[0].reshape(12) for s in batch], dtype=torch.float32)\n",
    "    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n",
    "    rewards     = torch.tensor([s[2] for s in batch], dtype=torch.int64)\n",
    "    next_states = torch.tensor([s[3].reshape(12) for s in batch], dtype=torch.float32)\n",
    "    game_end    = torch.tensor([s[4] for s in batch], dtype=torch.bool)\n",
    "    \n",
    "    \n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.int64) # the next state can be None if the game has ended\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "    \n",
    "    \n",
    "    # Compute all the Q values (forward pass)\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states)\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "        target_net.eval()\n",
    "        q_values_target = target_net(next_states)\n",
    "    next_state_max_q_values = torch.zeros(batch_size)\n",
    "    next_state_max_q_values[game_end] = q_values_target.max(dim=1)[0][game_end]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the Huber loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_epsilon(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "        \n",
    "    # With prob. epsilon perform a random action\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, 11)\n",
    "    else:    \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            state = torch.tensor(state.reshape(12), dtype=torch.float32) # Convert the state to tensor\n",
    "            net_out = net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        action = int(net_out.argmax())\n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "    return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating target network...\n",
      "EPISODE: 1 - FINAL SCORE: [9, -35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-ae2a97473510>:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  states      = torch.tensor([s[0].reshape(12) for s in batch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 101 - FINAL SCORE: [-13, -41]\n",
      "EPISODE: 201 - FINAL SCORE: [-6, -24]\n",
      "EPISODE: 301 - FINAL SCORE: [13, -7]\n",
      "EPISODE: 401 - FINAL SCORE: [-24, -18]\n",
      "EPISODE: 501 - FINAL SCORE: [-11, -17]\n",
      "EPISODE: 601 - FINAL SCORE: [7, -17]\n",
      "EPISODE: 701 - FINAL SCORE: [-30, -18]\n",
      "EPISODE: 801 - FINAL SCORE: [-2, -10]\n",
      "EPISODE: 901 - FINAL SCORE: [-45, 1]\n",
      "EPISODE: 1001 - FINAL SCORE: [-27, 13]\n",
      "EPISODE: 1101 - FINAL SCORE: [-24, -6]\n",
      "EPISODE: 1201 - FINAL SCORE: [-19, -9]\n",
      "EPISODE: 1301 - FINAL SCORE: [-24, 12]\n",
      "EPISODE: 1401 - FINAL SCORE: [-1, 1]\n",
      "EPISODE: 1501 - FINAL SCORE: [7, 3]\n",
      "EPISODE: 1601 - FINAL SCORE: [2, -12]\n",
      "EPISODE: 1701 - FINAL SCORE: [13, -19]\n",
      "EPISODE: 1801 - FINAL SCORE: [4, -18]\n",
      "EPISODE: 1901 - FINAL SCORE: [10, -14]\n",
      "EPISODE: 2001 - FINAL SCORE: [-3, 13]\n",
      "EPISODE: 2101 - FINAL SCORE: [-15, 11]\n",
      "EPISODE: 2201 - FINAL SCORE: [-8, -2]\n",
      "EPISODE: 2301 - FINAL SCORE: [-13, -13]\n",
      "EPISODE: 2401 - FINAL SCORE: [-37, 5]\n",
      "EPISODE: 2501 - FINAL SCORE: [11, -21]\n",
      "EPISODE: 2601 - FINAL SCORE: [-9, -21]\n",
      "EPISODE: 2701 - FINAL SCORE: [10, -54]\n",
      "EPISODE: 2801 - FINAL SCORE: [8, 6]\n",
      "EPISODE: 2901 - FINAL SCORE: [-7, -1]\n",
      "EPISODE: 3001 - FINAL SCORE: [9, 15]\n",
      "EPISODE: 3101 - FINAL SCORE: [-25, 29]\n",
      "EPISODE: 3201 - FINAL SCORE: [-2, -6]\n",
      "EPISODE: 3301 - FINAL SCORE: [-4, -2]\n",
      "EPISODE: 3401 - FINAL SCORE: [8, -26]\n",
      "EPISODE: 3501 - FINAL SCORE: [-23, -13]\n",
      "EPISODE: 3601 - FINAL SCORE: [6, -22]\n",
      "EPISODE: 3701 - FINAL SCORE: [-4, 6]\n",
      "EPISODE: 3801 - FINAL SCORE: [5, -11]\n",
      "EPISODE: 3901 - FINAL SCORE: [10, 6]\n",
      "EPISODE: 4001 - FINAL SCORE: [-80, -18]\n",
      "EPISODE: 4101 - FINAL SCORE: [15, -37]\n",
      "EPISODE: 4201 - FINAL SCORE: [8, 16]\n",
      "EPISODE: 4301 - FINAL SCORE: [-71, 25]\n",
      "EPISODE: 4401 - FINAL SCORE: [35, -5]\n",
      "EPISODE: 4501 - FINAL SCORE: [12, 6]\n",
      "EPISODE: 4601 - FINAL SCORE: [7, -45]\n",
      "EPISODE: 4701 - FINAL SCORE: [-5, -15]\n",
      "EPISODE: 4801 - FINAL SCORE: [9, 1]\n",
      "EPISODE: 4901 - FINAL SCORE: [-7, 17]\n",
      "Updating target network...\n",
      "EPISODE: 5001 - FINAL SCORE: [5, -17]\n",
      "EPISODE: 5101 - FINAL SCORE: [4, 22]\n",
      "EPISODE: 5201 - FINAL SCORE: [-22, 8]\n",
      "EPISODE: 5301 - FINAL SCORE: [21, -29]\n",
      "EPISODE: 5401 - FINAL SCORE: [10, -24]\n",
      "EPISODE: 5501 - FINAL SCORE: [20, 8]\n",
      "EPISODE: 5601 - FINAL SCORE: [10, 10]\n",
      "EPISODE: 5701 - FINAL SCORE: [0, -30]\n",
      "EPISODE: 5801 - FINAL SCORE: [11, 23]\n",
      "EPISODE: 5901 - FINAL SCORE: [20, 16]\n",
      "EPISODE: 6001 - FINAL SCORE: [1, -27]\n",
      "EPISODE: 6101 - FINAL SCORE: [25, -39]\n",
      "EPISODE: 6201 - FINAL SCORE: [-46, -10]\n",
      "EPISODE: 6301 - FINAL SCORE: [2, 12]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f17274ba4e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Update the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreplay_mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmin_samples_for_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-ae2a97473510>\u001b[0m in \u001b[0;36mupdate_step\u001b[1;34m(turn, policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtarget_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mq_values_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mnext_state_max_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnext_state_max_q_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame_end\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_values_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-505476f75667>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env = Mancala()\n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Initialize Deep-Q-Networks\n",
    "\n",
    "\n",
    "check = [0, 0]\n",
    "\n",
    "for episode_num in range(num_iterations):\n",
    "    \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Set epsilon \n",
    "    epsilon = max(epsilon_min, epsilon - epsilon_del)\n",
    "    \n",
    "    # Go on until the pole falls off\n",
    "    while not env.game_end:\n",
    "        # whose turn is it\n",
    "        if env.turn:\n",
    "            policy_net = policy_net_Bottom\n",
    "            target_net = target_net_Bottom\n",
    "            optimizer = optimizer_Bottom\n",
    "        else:\n",
    "            \n",
    "            policy_net = policy_net_Top\n",
    "            target_net = target_net_Top\n",
    "            optimizer = optimizer_Top\n",
    "            \n",
    "        # Save current env\n",
    "        state0  = env.copy()\n",
    "        \n",
    "        # Choose the action following the policy\n",
    "        action = choose_action_epsilon(policy_net, state0.board, epsilon)\n",
    "        \n",
    "        # Apply the action\n",
    "        reward = env.action(action)\n",
    "        \n",
    "        # Save state after action\n",
    "        state1 = env.copy()\n",
    "\n",
    "        # Here we can apply penalties for bad states or otherwise play with rewards\n",
    "    \n",
    "\n",
    "        # Update the replay memory\n",
    "        replay_mem.add(env.turn, state0, action, reward, state1)\n",
    "\n",
    "        # Update the network\n",
    "        if replay_mem.size[env.turn] > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "            update_step(env.turn, policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "        \n",
    "\n",
    "\n",
    "    # Update the target network every target_net_update_steps episodes\n",
    "    if episode_num % target_net_update_steps == 0:\n",
    "        print('Updating target network...')\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "    \n",
    "    check[0] += state1.score[0]\n",
    "    check[1] += state1.score[1]\n",
    "    \n",
    "    # Print the final score\n",
    "    if episode_num % 100 == 0:\n",
    "        print(f\"EPISODE: {episode_num + 1} - SCORE after 100 Episodes: {check}\") # Print the final score\n",
    "        check = [0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.linear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.memory[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net(torch.tensor(mem.memory[0][0][0], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = Mancala(True) # True for print output\n",
    "\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(10): \n",
    "    \n",
    "    print('##########################\\nNew Game\\n##########################')\n",
    "    \n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    \n",
    "    stop = 0\n",
    "    # Go on until the game is over\n",
    "    while not env.game_end and stop < 300:\n",
    "        print(env)\n",
    "        if env.turn:\n",
    "            net = target_net_Bottom\n",
    "        # Choose the best action (temperature 0)\n",
    "        action = int(net(torch.tensor(env.board.reshape(12), dtype=torch.float32)).argmax())\n",
    "        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "        env.action(action)\n",
    "        \n",
    "        \n",
    "        print(f'Current Score: {env.score}')\n",
    "        stop += 1\n",
    "    print('##########################\\nFinal Score\\n##########################')\n",
    "    print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
