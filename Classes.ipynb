{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Game Engine\n",
    "\n",
    "class Mancala:\n",
    "    def __init__(self, disp = False, seed = 0):\n",
    "        import random\n",
    "        # set seed \n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Initialize Mancala game board\n",
    "        self.board = [[4]*6 for i in range(2)]\n",
    "        \n",
    "        # Choose which player goes first (probably change to random initialization)\n",
    "        self.turn = random.randint(0, 1)    # 0 for TOP player\n",
    "                                            # 1 for BOTTOM player\n",
    "        \n",
    "        # Game End indicator\n",
    "        self.game_end = False\n",
    "        \n",
    "        # Should prints be displayed?\n",
    "        self.disp = disp\n",
    "        \n",
    "    \n",
    "    ## def output\n",
    "    def __str__(self):\n",
    "        return str(self.board[0]) + '\\n' + str(self.board[1]) +  'End = ' +  str(self.game_end)\n",
    "        \n",
    "    def seed(self, seed=0):\n",
    "        random.seed(seed)\n",
    "        \n",
    "    def valid_action(self, action, turn):\n",
    "        return self.board[turn][abs(5*(1 - turn) - action%6)] != 0\n",
    "    \n",
    "    ## act on the game\n",
    "    def action(self, action):\n",
    "        # Action meanings:\n",
    "        # 0-5:  distribute beans from bowl 0-5 WITHOUT placing one in the Score\n",
    "        # 6-11: distribute beans from bowl 0-5 AND place one in the Score\n",
    "        # 12: End game\n",
    "        \n",
    "        if action == 12:\n",
    "            self.game_end = True\n",
    "            return 0\n",
    "        \n",
    "        # interpret input\n",
    "        bowl = action % 6      # take beans from bowl\n",
    "        toScore = action // 6  # 0 -> skip Score, 1 -> do not skip Score\n",
    "        \n",
    "        # position\n",
    "        x = bowl if self.turn else 5 - bowl\n",
    "        y = self.turn\n",
    "        \n",
    "        # Init reward\n",
    "        reward = 0\n",
    "        \n",
    "        # How many beans are in chosen bowl\n",
    "        beans = self.board[y][x]\n",
    "        \n",
    "        # Who will be next?\n",
    "        next_turn = 1 - self.turn\n",
    "        \n",
    "        # If chosen bowl is empty\n",
    "        if beans == 0:\n",
    "            if self.disp: print(\"Error! You chose an empty bowl! -10 Points Penalty\")\n",
    "            reward -= 2\n",
    "            next_turn = 1 - next_turn\n",
    "            \n",
    "        # Take beans out of bowl\n",
    "        self.board[y][x] = 0\n",
    "        \n",
    "        # Beans are distributed counterclock-wise\n",
    "        direction = 2*self.turn - 1   # +1 -> right, -1 -> left\n",
    "        \n",
    "        # While there are beans left, distribute them counterclock-wise\n",
    "        while beans != 0:\n",
    "            \n",
    "            # If beans < 0 then there has been some error\n",
    "            if beans < 0:\n",
    "                if self.disp: print(\"Error nr of beans negative!\")\n",
    "                \n",
    "            # move to next bowl\n",
    "            x += direction  \n",
    "            \n",
    "            # if end is reached check if toScore is true and continue on other side\n",
    "            if x < 0 or x > 5:\n",
    "                # if toScore is true, put one bean in score (only if it is the correct Score)\n",
    "                if toScore and y == self.turn:\n",
    "                    reward += 1\n",
    "                    beans -= 1\n",
    "                    \n",
    "                    # if this was the last bean the same player will play again\n",
    "                    if beans == 0:\n",
    "                        next_turn = self.turn\n",
    "                        break\n",
    "                # change row and direction\n",
    "                y = 1 - y\n",
    "                direction = -direction\n",
    "                x += direction\n",
    "            \n",
    "            # if this was the last bean, check opposite bowl\n",
    "            if beans == 1 and self.board[y][x] == 0 and y == self.turn:\n",
    "                reward += 1 + self.board[1 - y][x]\n",
    "                self.board[1 - y][x] = 0\n",
    "            else:\n",
    "                self.board[y][x] += 1\n",
    "            beans -= 1\n",
    "            \n",
    "        # set next player\n",
    "        self.turn = next_turn\n",
    "        \n",
    "        return reward\n",
    "       \n",
    "    def end_check(self):\n",
    "        if self.game_end or self.board[self.turn] == [0]*6:\n",
    "            if self.disp: print(f'No more moves for {\"TOP\" if self.turn else \"BOTTOM\"}!')\n",
    "            self.__str__()\n",
    "            self.game_end = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def final_reward(self):\n",
    "        if not self.game_end:\n",
    "            if self.disp: print(\"The game is not over?!\")\n",
    "            return 0\n",
    "        \n",
    "        if self.disp: print(f'No more moves for {\"TOP\" if self.turn else \"BOTTOM\"} Player! \\nGame End\\n ==============================')\n",
    "        \n",
    "        return sum(self.board[1 - self.turn])\n",
    "        \n",
    "            \n",
    "    def copy(self):\n",
    "        cop = Mancala()\n",
    "        cop.board = np.array(self.board)\n",
    "        cop.game_end = bool(self.game_end)\n",
    "        cop.turn = int(self.turn)\n",
    "        return cop\n",
    "        \n",
    "    def reset(self, disp = False):\n",
    "        # Initialize Mancala game board\n",
    "        self.board = [[4]*6 for i in range(2)]\n",
    "        \n",
    "        # Choose which player goes first (probably change to random initialization)\n",
    "        self.turn = random.randint(0, 1)    # 0 for TOP player\n",
    "                                         # 1 for BOTTOM player\n",
    "            \n",
    "        # Game End indicator\n",
    "        self.game_end = False\n",
    "        \n",
    "        # Should prints be displayed?\n",
    "        self.disp = disp\n",
    "        \n",
    "    def flip(self):\n",
    "        self.board = [self.board[1][::-1], self.board[0][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Deep-Q-Learning we need a replay memory\n",
    "# This memory is for the Top player!\n",
    "class Memory:\n",
    "    def __init__(self, maxlen = 1e5):\n",
    "        import random\n",
    "        import copy\n",
    "        self.size = 0\n",
    "        self.memory = []\n",
    "        print(\"Memory Initialized\")\n",
    "        self.maxlen = int(maxlen)\n",
    "        self.current_pos = int(0)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx=-1):\n",
    "        if idx < 0 or idx >= self.size:\n",
    "            print(f'Index {idx} is too large for memory of length {self.size}. \\nInstead return random entry')\n",
    "            return self.memory[random.randint(0, self.size - 1)]\n",
    "        else:\n",
    "            return self.memory[idx]\n",
    "    \n",
    "    def draw(self):\n",
    "        if self.size: # self.size should not be 0\n",
    "            return self.__getitem__(random.randint(0, self.size - 1))\n",
    "        else:\n",
    "            print(f'Memory {turn} is not yet filled')\n",
    "            return []\n",
    "        \n",
    "    def draw_batch(self, batch_size):\n",
    "        batch_size = min(batch_size, self.size)\n",
    "        return [self.draw() for i in range(batch_size)]\n",
    "    \n",
    "    def add(self, previousState, action, reward, state, game_end):\n",
    "        if self.size < self.maxlen:\n",
    "            # relevant quantities: state0, action, reward, state1, game_end\n",
    "            # we need deepcopy the nested lists,\n",
    "            # because otherwise we only reference the list \n",
    "            # which will change later!\n",
    "            self.memory.append([copy.deepcopy(previousState), \n",
    "                                action, \n",
    "                                reward, \n",
    "                                copy.deepcopy(state),\n",
    "                                game_end])\n",
    "            self.size += 1\n",
    "            self.current_pos += 1\n",
    "        else:\n",
    "            self.current_pos = int(self.current_pos % self.maxlen)\n",
    "            self.memory[self.current_pos] = ([copy.deepcopy(previousState), \n",
    "                                              action, \n",
    "                                              reward, \n",
    "                                              copy.deepcopy(state),\n",
    "                                              game_end])\n",
    "            self.current_pos += 1\n",
    "        \n",
    "        \n",
    "    def add_random_game(self):\n",
    "        # Initialize game\n",
    "        game = Mancala(False)  \n",
    "        \n",
    "        # Start game Loop\n",
    "        while not game.game_end:\n",
    "            \n",
    "            # state t\n",
    "            game0 = game.copy()\n",
    "            \n",
    "            # random action\n",
    "            ac = random.randint(0, 11)\n",
    "            \n",
    "            while game.board[game.turn][abs(5*(1 - game.turn) - ac%6)] == 0:\n",
    "                #print(game.turn, game.board)\n",
    "                ac = random.randint(0, 11)\n",
    "            reward = game.action(ac)\n",
    "            \n",
    "            # save state(t), action, state(t+1)\n",
    "            self.add(game0.turn, game0, ac, reward, game.copy())  \n",
    "            \n",
    "    def __str__(self):\n",
    "        return f'Size: {self.size} \\nFirst Entry Player 0:\\n {self.memory[0][0]} \\n... \\nLast Entry Player 1:\\n {self.memory[1][self.size[1] - 1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \n",
    "    def __init__(self, pos, name):\n",
    "        \n",
    "        import numpy as np\n",
    "        import random\n",
    "        \n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        self.name = name\n",
    "        \n",
    "        self.pos = pos\n",
    "        self.position = [\"TOP\", \"BOTTOM\"][pos]\n",
    "        \n",
    "        self.previousState = [[4]*6, [4]*6]\n",
    "        self.action = -1\n",
    "        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.score = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.previousState = [[4]*6, [4]*6]\n",
    "        self.action = -1\n",
    "        \n",
    "        \n",
    "        \n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self, pos=1, name=\"Random\"):\n",
    "        Player.__init__(self, pos, name)\n",
    "        self.memory.maxlen = 2\n",
    "    \n",
    "    def think(self, game):\n",
    "        action = random.randint(0, 11)\n",
    "        while not game.valid_action(action, game.turn):\n",
    "            action = random.randint(0, 11)\n",
    "        self.action = action\n",
    "        \n",
    "        return action\n",
    "        \n",
    "class GreedyPlayer(Player):\n",
    "    def __init__(self, pos=1, name=\"Greedy\"):\n",
    "        Player.__init__(self, pos, name)\n",
    "        self.reward_list = np.zeros(12, dtype=np.int64)\n",
    "        self.memory.maxlen = 2\n",
    "    \n",
    "    def think(self, game):\n",
    "        reward_list = np.zeros(12, dtype=np.int64)\n",
    "        #if 0 not in game.board[self.pos]:\n",
    "        #    if 12 not in game.board[self.pos] and 13 not in game.board[self.pos]:\n",
    "        #        return random.randint(0, 11)\n",
    "                \n",
    "        if True: #0\n",
    "            for idx, beans in enumerate(game.board[self.pos][::(game.turn*2-1)]):\n",
    "                # If there are no beans, skip\n",
    "                if beans == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # if beans reach 'score', and action >= 6, put one in 'score'\n",
    "                if beans + idx >= 5:\n",
    "                    reward_list[idx + 6] += 1\n",
    "                    if beans + idx > 17:\n",
    "                        reward_list[idx + 6] += 1\n",
    "                \n",
    "                # if beans 13 it cannot land in an empty bowl\n",
    "                if beans > 13:\n",
    "                    continue\n",
    "                # if beans=13 then action idx+6 will be good\n",
    "                elif beans == 13:\n",
    "                    reward_list[idx + 6] += 2 + game.board[1 - self.pos][abs(5*(1-game.turn)-idx)]\n",
    "                # if beans=12 then action idx will be good\n",
    "                elif beans == 12:\n",
    "                    reward_list[idx] += 2 + game.board[1 - self.pos][abs(5*(1-game.turn)-idx)]\n",
    "                # if beans do not cross 'score'\n",
    "                elif beans + idx <= 5:\n",
    "                    if game.board[self.pos][abs(5*(1-game.turn)-idx-beans)] == 0:\n",
    "                        reward_list[idx] += 1 + game.board[1 - self.pos][abs(5*(1-game.turn)-idx-beans)]\n",
    "                        reward_list[idx + 6] += 1 + game.board[1 - self.pos][abs(5*(1-game.turn)-idx-beans)]\n",
    "                # if beans cross 'score' of both players\n",
    "                elif beans + idx == 12:\n",
    "                    if game.board[self.pos][abs(5*game.turn)] == 0:\n",
    "                        reward_list[idx] += 2 + game.board[1 - self.pos][abs(5*game.turn)]\n",
    "                elif beans + idx > 12:\n",
    "                    if game.board[self.pos][abs(5*(1-game.turn)-(beans + idx - 12))] == 0:\n",
    "                        reward_list[idx] += 2 + game.board[1 - self.pos][abs(5*(1-game.turn)-(beans + idx - 12))]\n",
    "                    if game.board[self.pos][abs(5*(1-game.turn)-(beans + idx - 13))] == 0:\n",
    "                        reward_list[idx + 6] += 2 + game.board[1 - self.pos][abs(5*(1-game.turn)-(beans + idx - 13))]\n",
    "            \n",
    "        else:\n",
    "            for idx, beans in enumerate(game.board[self.pos]):\n",
    "                if beans == 0:\n",
    "                    continue\n",
    "                \n",
    "                if beans > idx:\n",
    "                    reward_list[idx + 6] += 1\n",
    "                    if beans > 12 + idx:\n",
    "                        reward_list[idx + 6] += 1\n",
    "                        \n",
    "                if beans > 13:\n",
    "                    continue\n",
    "                elif beans == 13:\n",
    "                    reward_list[idx + 6] = 2 + game.board[1 - self.pos][idx]\n",
    "                elif beans == 12:\n",
    "                    reward_list[idx] = 2 + game.board[1 - self.pos][idx]\n",
    "                elif beans + idx <= 5:\n",
    "                    if game.board[self.pos][idx - beans] == 0:\n",
    "                        reward_list[idx] = 1 + game.board[1 - self.pos][idx - beans]\n",
    "                        reward_list[idx + 6] = 1 + game.board[1 - self.pos][idx - beans]\n",
    "                elif beans + idx == 12:\n",
    "                    if game.board[self.pos][beans + (5-idx) - 12] == 0:\n",
    "                        reward_list[idx] = 2 + game.board[1 - self.pos][beans - idx - 7]\n",
    "                elif beans+ idx > 12:\n",
    "                    if game.board[self.pos][beans + (5-idx) - 12] == 0:\n",
    "                        reward_list[idx] = 2 + game.board[1 - self.pos][beans - idx - 7]\n",
    "                    if game.board[self.pos][beans + (5-idx) - 13] == 0:\n",
    "                        reward_list[idx + 6] = 2 + game.board[1 - self.pos][beans - idx - 8]\n",
    "                        \n",
    "        #print(reward_list)\n",
    "        action = np.random.choice(np.where(reward_list == np.max(reward_list))[0])\n",
    "        while not game.valid_action(action, game.turn):\n",
    "            #print(reward_list, \"invalid action\", action)\n",
    "            reward_list[action] -= 1000\n",
    "            action = np.random.choice(np.where(reward_list == np.max(reward_list))[0])\n",
    "        #print(action)\n",
    "        return action\n",
    "    \n",
    "\n",
    "class HumanPlayer(Player):\n",
    "    def __init__(self, pos=1, name='Human'):\n",
    "        Player.__init__(self, pos, name)\n",
    "        self.memory.maxlen = 2\n",
    "        \n",
    "    def think(self, game):\n",
    "        #print(game)\n",
    "        \n",
    "        inp = int(input(f'It is {self.name}s turn. Whats your next move? (input 0-11, end: 12)'))\n",
    "        if inp in range(13):\n",
    "            action = inp\n",
    "        else:\n",
    "            print(f'{inp} is an invalid input! Valid inputs are numbers 0-11 for actions and 12 to end the game.')\n",
    "            action = self.think(game)\n",
    "            \n",
    "        self.action = action\n",
    "        \n",
    "        return action\n",
    "        \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        from torch import nn\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "                nn.Linear(state_space_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, action_space_dim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "        \n",
    "class DQNPlayer(Player):\n",
    "    def __init__(self, \n",
    "                 gamma=0.97, \n",
    "                 learning_rate = 1e-2,\n",
    "                 batch_size = 256,\n",
    "                 target_net_update_steps = 1000,\n",
    "                 replay_memory_capacity = 1e5, \n",
    "                 epsilon_start = 100,\n",
    "                 epsilon_stop = 3e3,\n",
    "                 epsilon_min = 0.05,\n",
    "                 weight_decay = 1e-5,\n",
    "                 policy = 'softmax',\n",
    "                 pos=0, \n",
    "                 name='AI'):\n",
    "        import torch\n",
    "        import random\n",
    "        import numpy as np\n",
    "        from torch import nn\n",
    "        import math\n",
    "        \n",
    "        Player.__init__(self, pos, name)\n",
    "        \n",
    "        ### Set Memory length\n",
    "        self.memory.maxlen = replay_memory_capacity\n",
    "        \n",
    "        ### Initialize the policy network\n",
    "        self.net = DQN(state_space_dim=12, action_space_dim=12)\n",
    "        \n",
    "        ### Initialize the target network with the same weights of the policy network\n",
    "        self.target_net = DQN(state_space_dim=12, action_space_dim=12)\n",
    "        self.target_net_update_steps = target_net_update_steps\n",
    "        \n",
    "        ### Initialize the optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate, weight_decay=1e-4) # The optimizer will update ONLY the parameters of the policy network\n",
    "        \n",
    "        ### Initialize the loss function (Huber loss)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.loss = 0\n",
    "        \n",
    "        self.idx = 0\n",
    "        \n",
    "        self.final = False\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "        if False:\n",
    "            torch.manual_seed(0)\n",
    "            np.random.seed(0)\n",
    "            random.seed(0)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.memory.maxlen = replay_memory_capacity\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_stop = epsilon_stop\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "    def epsilon(self):\n",
    "        return max(self.epsilon_min, \n",
    "                   1 if self.idx < self.epsilon_start \n",
    "                   else 1 - (1 - self.epsilon_min) * (self.idx - self.epsilon_start)  / (self.epsilon_stop - self.epsilon_start))\n",
    "        \n",
    "    def update_step(self):\n",
    "        \n",
    "        if self.idx < self.epsilon_start:\n",
    "            self.idx += 1\n",
    "            pass\n",
    "        \n",
    "        #replay_mem = self.memory\n",
    "        #policy_net = self.net\n",
    "        #target_net = self.target_net\n",
    "        #gamma = self.gamma\n",
    "        #optimizer = self.optimizer\n",
    "        #loss_fn = self.loss_fn\n",
    "        #batch_size = self.batch_size\n",
    "        \n",
    "        # Sample the data from the replay memory\n",
    "        batch = self.memory.draw_batch(self.batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Create tensors for each element of the batch\n",
    "        states      = torch.tensor([s[0][0] + s[0][1] for s in batch], dtype=torch.float32)\n",
    "        actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n",
    "        rewards     = torch.tensor([s[2] for s in batch], dtype=torch.int64)\n",
    "        next_states = torch.tensor([s[3][0] + s[3][1] if not s[4] else [0]*12 for s in batch], dtype=torch.float32)\n",
    "        game_end    = torch.tensor([s[4] for s in batch], dtype=torch.bool)\n",
    "\n",
    "\n",
    "        # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "        non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.int64) # the next state can be None if the game has ended\n",
    "        non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "\n",
    "\n",
    "        # Compute all the Q values (forward pass)\n",
    "        self.net.train()\n",
    "        q_values = self.net(states)\n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            q_values_target = self.target_net(next_states)\n",
    "        next_state_max_q_values = torch.zeros(batch_size)\n",
    "        next_state_max_q_values[game_end] = q_values_target.max(dim=1)[0][game_end]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = rewards + (next_state_max_q_values * self.gamma)\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        self.loss = self.loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), 2)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.idx += 1\n",
    "        \n",
    "        # Update the target network every target_net_update_steps episodes\n",
    "        if self.idx == self.epsilon_start + 100 or self.idx % self.target_net_update_steps == 0:\n",
    "            print('Updating target network...')\n",
    "            self.target_net.load_state_dict(self.net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "    \n",
    "    def think(self, game, invalid = []):\n",
    "        if self.policy == 'softmax':\n",
    "            return self.think_softmax(game, invalid)\n",
    "        else:\n",
    "            return self.think_epsilon(game, invalid)\n",
    "    \n",
    "    def think_epsilon(self, game, invalid = []):\n",
    "        \n",
    "        if not self.final and random.random() < self.epsilon():\n",
    "            action = random.randint(0, 11)\n",
    "            while action in invalid:\n",
    "                action = random.randint(0, 11)\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.net.eval()\n",
    "                net_out = self.net(torch.tensor([game.board[0] + game.board[1]], dtype=torch.float32))\n",
    "                action = int(torch.argmax(net_out))\n",
    "                \n",
    "            \n",
    "            while action in invalid:\n",
    "                #print(reward_list, \"invalid action\", action)\n",
    "                net_out[0][action] -= 1000\n",
    "                action = int(torch.argmax(net_out))\n",
    "        \n",
    "        \n",
    "        return int(action)\n",
    "        \n",
    "    def think_softmax(self, game, invalid=[]):  #ignore softmax temperature option\n",
    "        \n",
    "        # ignore softmax temperature option\n",
    "        # instead use epsilon and softmax combined.\n",
    "        # Lets see if it works.\n",
    "        \n",
    "        if not self.final and random.random() < self.epsilon():\n",
    "            action = random.randint(0, 11)\n",
    "            while action in invalid:\n",
    "                action = random.randint(0, 11)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.net.eval()\n",
    "            net_out = self.net(torch.tensor([game.board[0] + game.board[1]], dtype=torch.float32))\n",
    "            net_softmax = torch.softmax(net_out, 1, dtype=torch.float64)[0]\n",
    "            net_softmax /= net_softmax.sum()\n",
    "            if self.final:\n",
    "                action = int(torch.argmax(net_softmax))\n",
    "            else:\n",
    "                action = int(np.random.choice(list(range(12)), p=net_softmax))\n",
    "            \n",
    "        while action in invalid:\n",
    "            net_out[0][action] -= 1000\n",
    "            net_softmax = torch.softmax(net_out, 1, dtype=torch.float64)[0]\n",
    "            net_softmax /= net_softmax.sum()\n",
    "            if self.final:\n",
    "                action = int(torch.argmax(net_softmax))\n",
    "            else:\n",
    "                action = int(np.random.choice(list(range(12)), p=net_softmax))\n",
    "        \n",
    "        return int(action)\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Arena():\n",
    "    def __init__(self, game, aPlayer, bPlayer, disp = False):\n",
    "        \n",
    "        import random\n",
    "        import numpy as np\n",
    "        \n",
    "        self.game = game\n",
    "        \n",
    "        if aPlayer.pos == bPlayer.pos:\n",
    "            print(\"Both Players have the same position!\")\n",
    "            self.player = [aPlayer, bPlayer]\n",
    "            self.game.game_end = True\n",
    "        else:\n",
    "            self.player = [aPlayer, bPlayer][::bPlayer.pos - aPlayer.pos]\n",
    "        \n",
    "        \n",
    "        self.disp = disp\n",
    "        \n",
    "        if self.disp:\n",
    "            print(\"Game initialized!\")\n",
    "            print(self.__str__())\n",
    "            \n",
    "    def __str__(self):\n",
    "        out = f'Top Player:    {self.player[0].name}. Score: {self.player[0].score}\\n'\n",
    "        out += f'Bottom Player: {self.player[1].name}. Score: {self.player[1].score}\\n'\n",
    "        out += str(self.game)\n",
    "        out += f'\\nNext Player: {[self.player[0].name, self.player[1].name][self.game.turn]}'\n",
    "        return out if self.disp else ''\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        for p in self.player:\n",
    "            p.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        # Whose turn is it?\n",
    "        player = self.player[self.game.turn]\n",
    "        \n",
    "        # Put what happened since the last move into Player's Memory\n",
    "        if player.action != -1:\n",
    "            player.memory.add(player.previousState, \n",
    "                              player.action, \n",
    "                              player.reward, \n",
    "                              self.game.board,\n",
    "                              self.game.game_end)\n",
    "        # Reset reward\n",
    "        player.reward = 0\n",
    "        \n",
    "        # Save current board for future memory\n",
    "        player.previousState = self.game.board\n",
    "        \n",
    "        # Let player choose action\n",
    "        player.action = player.think(self.game)\n",
    "        \n",
    "        # Check if this was a valid move\n",
    "        invalid = []\n",
    "        while not self.game.valid_action(player.action, player.pos):\n",
    "            # print('AI chose invalid action')\n",
    "            # Save this wrong move with a penalty of -10!\n",
    "            player.memory.add(self.game.board,\n",
    "                             player.action,\n",
    "                             -10,\n",
    "                             self.game.board,\n",
    "                             False)\n",
    "            invalid.append(player.action)\n",
    "            player.action = player.think(self.game, invalid)\n",
    "        \n",
    "        \n",
    "        # If AI, print net output\n",
    "        if self.disp and player.name == 'AI':\n",
    "            print(list(player.net(torch.tensor([self.game.board[0] + self.game.board[0]], dtype=torch.float32))))\n",
    "        \n",
    "        # Display choice\n",
    "        if self.disp: print(f'{player.name} chooses {player.action}')\n",
    "        \n",
    "            \n",
    "        # Perform action and get reward from Game\n",
    "        reward = self.game.action(player.action)\n",
    "        \n",
    "        # Display reward\n",
    "        if self.disp: print(f'Reward: {reward}')\n",
    "        \n",
    "        # player gets positive reward\n",
    "        player.reward += reward\n",
    "        player.score += reward\n",
    "        \n",
    "        # opposite player gets negative reward\n",
    "        self.player[1 - player.pos].reward -= reward\n",
    "        \n",
    "        # check if game is over: \n",
    "        if self.game.end_check():\n",
    "            reward = self.game.final_reward()\n",
    "            self.player[self.game.turn].reward -= reward\n",
    "            self.player[1 - self.game.turn].reward += reward\n",
    "            self.player[1 - self.game.turn].score += reward\n",
    "            \n",
    "            self.player[self.game.turn].memory.add(self.player[self.game.turn].previousState,\n",
    "                                                   self.player[self.game.turn].action,\n",
    "                                                   self.player[self.game.turn].reward,\n",
    "                                                   None,\n",
    "                                                   self.game.game_end)\n",
    "            \n",
    "            self.player[1 - self.game.turn].memory.add(self.player[1 - self.game.turn].previousState,\n",
    "                                                   self.player[1 - self.game.turn].action,\n",
    "                                                   self.player[1 - self.game.turn].reward,\n",
    "                                                   None,\n",
    "                                                   self.game.game_end)\n",
    "            if self.disp: print('Game Over')\n",
    "            \n",
    "        else:\n",
    "            if self.disp: print(self.__str__())\n",
    "            self.play()\n",
    "            \n",
    "    def test(self, player0, player1, num_games=500, disp=False):\n",
    "        testarena = Arena(self.game, player0, player1, disp=disp)\n",
    "        \n",
    "        player0.final = True\n",
    "        player1.final = True\n",
    "        \n",
    "        res = [[],[]]\n",
    "        \n",
    "        ## Play some games\n",
    "        for i in range(num_games):\n",
    "            testarena.reset()\n",
    "            testarena.play()\n",
    "            res[0].append(player0.score)\n",
    "            res[1].append(player1.score)\n",
    "            \n",
    "        print(f'Average Score: Top {player0.name}: {sum(res[0])/len(res[0])} - Bottom {player1.name}: {sum(res[1])/len(res[1])}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        player0.final=False\n",
    "        player1.final=False\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
